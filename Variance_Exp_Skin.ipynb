{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3575733f",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "#### figure out a way to make the model deeper \n",
    "#### consider small model as low rank representation and project higher. (using outer product) train only small number of outer product parameters - https://arxiv.org/pdf/2012.13255.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73ab1cb2",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import wandb\n",
    "   \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import wandb\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f875dfa8",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#Link to dataset: https://archive.ics.uci.edu/ml/datasets/DeliciousMIL%3A+A+Data+Set+for+Multi-Label+Multi-Instance+Learning+with+Instance+Labels#\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def createDataset(path):\n",
    "    file = open(path, \"r\")\n",
    "    dataset = []\n",
    "    split = file.read().splitlines()\n",
    " \n",
    "    for i, line in enumerate(split):\n",
    "        row = line.split('\\t')\n",
    "        while \"\" in row:\n",
    "            row.remove(\"\")\n",
    "        for i, item in enumerate(row): \n",
    "            try:\n",
    "                row[i] = float(item.split(\":\")[1])\n",
    "            except:\n",
    "                row[i] = int(item)\n",
    "        dataset.append(row)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd9c17e5",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(245057, 4)\n",
      "(51444, 4)\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.DataFrame(createDataset('Skin_NonSkin.txt'))\n",
    "dataset.dropna(inplace=True)\n",
    "print(dataset.shape)\n",
    "dataset=dataset.drop_duplicates()\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ccab5b32",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74</td>\n",
       "      <td>85</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73</td>\n",
       "      <td>84</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72</td>\n",
       "      <td>83</td>\n",
       "      <td>121</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>81</td>\n",
       "      <td>119</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>69</td>\n",
       "      <td>80</td>\n",
       "      <td>118</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244672</th>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>49</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244703</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244719</th>\n",
       "      <td>62</td>\n",
       "      <td>64</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244720</th>\n",
       "      <td>58</td>\n",
       "      <td>61</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244848</th>\n",
       "      <td>103</td>\n",
       "      <td>106</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51444 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0    1    2  3\n",
       "0        74   85  123  1\n",
       "1        73   84  122  1\n",
       "2        72   83  121  1\n",
       "3        70   81  119  1\n",
       "5        69   80  118  1\n",
       "...     ...  ...  ... ..\n",
       "244672   73   73   49  2\n",
       "244703   19   19   19  2\n",
       "244719   62   64   35  2\n",
       "244720   58   61   29  2\n",
       "244848  103  106   51  2\n",
       "\n",
       "[51444 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89eb5290",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class skin(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87792852",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          1    2  3\n",
      "0        85  123  1\n",
      "1        84  122  1\n",
      "2        83  121  1\n",
      "3        81  119  1\n",
      "5        80  118  1\n",
      "...     ...  ... ..\n",
      "244672   73   49  2\n",
      "244703   19   19  2\n",
      "244719   64   35  2\n",
      "244720   61   29  2\n",
      "244848  106   51  2\n",
      "\n",
      "[51444 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dataset.loc[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04643aac",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train shape :  (41155, 3)\n",
      "y train label shape :  (41155,)\n",
      "X valid  shape :  (10289, 3)\n",
      "y valid shape:  (10289,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler  \n",
    "\n",
    "X_train, X_valid, y_train, y_valid = \\\n",
    "    train_test_split(dataset.loc[:, :2], dataset.loc[:, 3:], test_size=0.2, random_state=42)\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_valid = scaler.transform(X_valid)\n",
    "\n",
    "y_train = y_train - 1\n",
    "y_valid = y_valid - 1\n",
    "\n",
    "y_train = y_train.squeeze()\n",
    "y_valid = y_valid.squeeze()\n",
    "\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_valid, y_valid = np.array(X_valid), np.array(y_valid)\n",
    "\n",
    "\n",
    "\n",
    "print('X train shape : ', X_train.shape)\n",
    "print('y train label shape : ', y_train.shape)\n",
    "print('X valid  shape : ', X_valid.shape)\n",
    "print('y valid shape: ', y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "46dd35ec",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 0, 1])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "512e95d8",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = skin(X=X_train.astype(float), y=y_train)\n",
    "val_dataset = skin(X=X_valid.astype(float), y=y_valid)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=len(train_dataset), shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "56b64256",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "X = torch.tensor(train_dataset.X, device = device)\n",
    "y = torch.tensor(train_dataset.y, device = device)\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 512\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_FEATURES = X.shape[1]\n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d32374eb",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1],\n",
      "        [1],\n",
      "        [1],\n",
      "        ...,\n",
      "        [1],\n",
      "        [0],\n",
      "        [1]])\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "43e55274",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'grid'\n",
    "    }\n",
    "\n",
    "parameters_dict = {\n",
    "    \"LEARNING_RATE\": {\n",
    "        'value': LEARNING_RATE\n",
    "        },\n",
    "    \"NUM_FEATURES\": {\n",
    "        'value': NUM_FEATURES\n",
    "        },\n",
    "    \"NUM_CLASSES\": {\n",
    "        'value': NUM_CLASSES\n",
    "        },\n",
    "    \"EPOCHS\": {\n",
    "        'value': EPOCHS\n",
    "        },\n",
    "    }\n",
    "# \n",
    "sweep_config['parameters'] = parameters_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "dfd76aa5",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: gwuk8cde\n",
      "Sweep URL: https://wandb.ai/rice-and-shine/Data_Variance_Exp/sweeps/gwuk8cde\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"Data_Variance_Exp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "26083968",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class MulticlassClassification(nn.Module):\n",
    "    def __init__(self, num_feature, num_class):\n",
    "        super(MulticlassClassification, self).__init__()\n",
    "        \n",
    "        self.layer_1 = nn.Linear(num_feature, 2048)\n",
    "        self.layer_2 = nn.Linear(2048, 512)\n",
    "        self.layer_3 = nn.Linear(512, 128)\n",
    "        self.layer_out = nn.Linear(128, num_class)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.batchnorm1 = nn.InstanceNorm1d(2048)\n",
    "        self.batchnorm2 = nn.InstanceNorm1d(512)\n",
    "        self.batchnorm3 = nn.InstanceNorm1d(128)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.layer_2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.layer_3(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        \n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f8fdb2d7",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# neighbors = torch.linalg.norm(X.unsqueeze(1) - X.unsqueeze(0), dim=2).argsort(dim=1)[:, :33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "66d7d00d",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 38808, 27056,  ..., 12742,  5437, 18427],\n",
       "        [    1, 38909, 25967,  ..., 24233, 11397,  4644],\n",
       "        [    2, 37792, 40271,  ...,  9303, 32798, 22147],\n",
       "        ...,\n",
       "        [41152,  1855, 23863,  ..., 21431,  5212, 21239],\n",
       "        [41153, 33173, 36497,  ..., 39413, 18052,  6038],\n",
       "        [41154, 12283, 26185,  ..., 27497, 17250, 31485]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "00ddd3fc",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "def compute_loss_stateless_model (params, buffers, sample, target):\n",
    "    batch = sample.unsqueeze(0)\n",
    "    targets = target.unsqueeze(0)\n",
    "\n",
    "    predictions = fmodel(params, buffers, batch) \n",
    "    loss = torch.nn.functional.cross_entropy(predictions, targets)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def compute_next_batch(X, y, batch_norm):\n",
    "    highest_grad_points = batch_norm.argsort(descending=True)[:32]\n",
    "    next_batch = X[highest_grad_points]\n",
    "    # neighbors = torch.linalg.norm(X.unsqueeze(0) - next_batch.unsqueeze(1), dim=2).argsort(dim=1)[:, :32]\n",
    "    # neigh_norms = 0\n",
    "    # for neigh  in neighbors:\n",
    "    #     neigh_grads = batch_norm[neigh]\n",
    "    #     neigh_norms += sum(neigh_grads)/len(neigh_grads)\n",
    "    \n",
    "    # neigh_norms /= len(neighbors)\n",
    "\n",
    "\n",
    "    x0=next_batch[y==0]\n",
    "    x1=next_batch[y==1]\n",
    "\n",
    "    batch_norm0=batch_norm[y==0]\n",
    "    batch_norm1=batch_norm[y==1]\n",
    "\n",
    "    next_batch_0 = torch.linalg.norm(X.unsqueeze(0)-x0.unsqueeze(1),dim=2).argsort(dim=1)[:,1:32+1]\n",
    "    next_batch_0=torch.reshape(next_batch_0,(-1,))\n",
    "\n",
    "    next_batch_1 = torch.linalg.norm(X.unsqueeze(0)-x1.unsqueeze(1),dim=2).argsort(dim=1)[:,1:32+1]\n",
    "    next_batch_1=torch.reshape(next_batch_1,(-1,))\n",
    "\n",
    "    \n",
    "    \n",
    "    next_batch_weights_0=torch.cat(tuple(batch_norm0.repeat(32,1).T))\n",
    "    next_batch_weights_1=torch.cat(tuple(batch_norm1.repeat(32,1).T))\n",
    "\n",
    "\n",
    "    ## weighted selection of next batch.\n",
    "    next_batch_0 = next_batch_0[next_batch_weights_0.multinomial(32)]\n",
    "    next_batch_1 = next_batch_1[next_batch_weights_1.multinomial(32)]\n",
    "    neigh_norms = 0\n",
    "    for neigh  in next_batch_0:\n",
    "        neigh_grads = batch_norm[neigh]\n",
    "        neigh_norms += sum(neigh_grads)/len(neigh_grads)\n",
    "    \n",
    "    \n",
    "\n",
    "    for neigh  in next_batch_1:\n",
    "        neigh_grads = batch_norm[neigh]\n",
    "        neigh_norms += sum(neigh_grads)/len(neigh_grads)\n",
    "    \n",
    "    neigh_norms /= (len(next_batch_1)+len(next_batch_0))\n",
    "    \n",
    "    # del neighbors\n",
    "\n",
    "    del highest_grad_points\n",
    "    gc.collect()\n",
    "    return neigh_norms\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e452af48",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MulticlassClassification(\n",
       "  (layer_1): Linear(in_features=3, out_features=2048, bias=True)\n",
       "  (layer_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (layer_3): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (layer_out): Linear(in_features=128, out_features=2, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (batchnorm1): InstanceNorm1d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  (batchnorm2): InstanceNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  (batchnorm3): InstanceNorm1d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       ")"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "from functorch import make_functional_with_buffers, vmap, grad\n",
    "\n",
    "base_model = MulticlassClassification(num_feature = NUM_FEATURES, num_class=NUM_CLASSES)\n",
    "base_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fc3a4811",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "ft_compute_grad = grad(compute_loss_stateless_model)\n",
    "ft_compute_sample_grad = vmap(ft_compute_grad, in_dims=(None, None, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f205e4e3",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "loss_stats = {\n",
    "    'train': [],\n",
    "    \"val\": [],\n",
    "    \"grad\": [],\n",
    "    \"var-1\":[],\n",
    "    \"var-2\":[],\n",
    "    \"var-3\":[],\n",
    "    \"var-4\":[],\n",
    "    \"var-5\":[],\n",
    "    \"var-6\":[],\n",
    "    \"var-7\":[],\n",
    "    \"var-8\":[],\n",
    "    \"var-9\":[],\n",
    "    \"var-10\":[],\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "155d202a",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a4ff4de9",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def get_variance_batch(max_grad_data_point,y_max_grad_data_point):\n",
    "\n",
    "    new_batch=[]\n",
    "    # print(max_grad_data_point.shape)\n",
    "    d=max_grad_data_point.shape[1]\n",
    "\n",
    "    for i in range(1,11):\n",
    "        for j in range(100):\n",
    "            random_point=np.random.rand(d)\n",
    "            norm=np.linalg.norm(random_point)\n",
    "            random_point=random_point*(i/norm)\n",
    "            new_batch.append(random_point)\n",
    "            # print(torch.linalg.norm(random_point))\n",
    "    return torch.tensor(new_batch),torch.tensor([y_max_grad_data_point]*1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "608274cf",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "l,m=get_variance_batch(X[[222]],y[[222]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5fc02f87",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def wandb_trainer_function(config=None):\n",
    "    # INITIALIZE NEW WANDB RUN\n",
    "    with wandb.init(config=config) as run:\n",
    "        #USING THE CONFIG TO SET THE HYPERPARAMETERS FOR EACH RUN\n",
    "        config = wandb.config\n",
    "        wandb.define_metric(\"custom_step\")\n",
    "        wandb.define_metric(\"Neighbor Gradient\", step_metric='custom_step')\n",
    "        wandb.define_metric(\"Average Gradient\", step_metric='custom_step')\n",
    "        wandb.define_metric(\"Train Loss\", step_metric='custom_step')\n",
    "        wandb.define_metric(\"Val Loss\", step_metric='custom_step')\n",
    "        \n",
    "\n",
    "        run.name = \"NN-Covtype-batched\"\n",
    "\n",
    "        model = copy.deepcopy(base_model)\n",
    "\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "        val_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "        global fmodel\n",
    "\n",
    "        print(\"Begin training.\")\n",
    "        train_epoch_loss=0\n",
    "        val_epoch_loss = 0\n",
    "        gradient_norm_epoch = 0\n",
    "        for e in tqdm(range(1, EPOCHS+1)):\n",
    "            torch.cuda.empty_cache()\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            pred = model(X.float())\n",
    "\n",
    "            train_loss = criterion(pred, y)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            fmodel, params, buffers = make_functional_with_buffers(model)\n",
    "\n",
    "            ### use functorch to calculate per sample gradient\n",
    "            ft_per_sample_grads = ft_compute_sample_grad(params, buffers, X.float(), y)\n",
    "\n",
    "            ### calculate norm of the gradient and use it to compute next batch\n",
    "            batch_norm = torch.zeros(X.size(0), device=device)\n",
    "            for item in ft_per_sample_grads:\n",
    "                batch_norm +=  torch.linalg.norm(item, dim=tuple(range(1,len(item.shape))))\n",
    "            gradient_norm_epoch = batch_norm.sum().item()/X.size(0)\n",
    "\n",
    "            ###CODE FOR VARIANCE BATCH EXPERIMENT STARTS HERE\n",
    "            max_grad_data_point=batch_norm.argsort(descending=True)[0:1]\n",
    "            \n",
    "            variance_batch, y_max_grad_data_point=get_variance_batch(X[max_grad_data_point],y[max_grad_data_point])\n",
    "\n",
    "            ft_per_sample_grads_exp = ft_compute_sample_grad(params, buffers, variance_batch.float(), y_max_grad_data_point)\n",
    "\n",
    "            batch_norm_exp = torch.zeros(variance_batch.size(0), device=device)\n",
    "            for item in ft_per_sample_grads_exp:\n",
    "                batch_norm_exp +=  torch.linalg.norm(item, dim=tuple(range(1,len(item.shape))))\n",
    "            \n",
    "            gradient_norm_epoch_exp=torch.zeros(10, device=device)\n",
    "\n",
    "            for i in range(10):\n",
    "                gradient_norm_epoch_exp[i]=batch_norm_exp[i:i+100].sum().item()/100\n",
    "                loss_stats[f'var-{i+1}']=gradient_norm_epoch_exp[i]\n",
    "\n",
    "            ###CODE FOR VARIANCE BATCH EXPERIMENT ENDS HERE\n",
    "            del ft_per_sample_grads\n",
    "            del fmodel\n",
    "            del params\n",
    "            del buffers\n",
    "            gc.collect()\n",
    "            \n",
    "            ## compute neighbors of points that give largest gradients, and see if their gradients are higher in general\n",
    "            # neighbor_norms = compute_next_batch(X, y, batch_norm)\n",
    "\n",
    "\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                for X_val, y_val in val_loader:\n",
    "                    val_pred = model(X_val.float())\n",
    "                    val_loss = criterion(val_pred, y_val)\n",
    "                    val_epoch_loss += val_loss.item()\n",
    "\n",
    "\n",
    "                # TRAIN LOSS AND ACCURACY\n",
    "                loss_stats['train'].append(train_loss)\n",
    "                loss_stats['val'].append(val_epoch_loss/len(val_loader))\n",
    "                loss_stats['grad'].append(gradient_norm_epoch)\n",
    "\n",
    "\n",
    "                ## plot val loss and accuracy here. For train, standardise a subset for loss/accuracy\n",
    "                print(f'Epoch {e+0:03}: | Train Loss: {loss_stats[\"train\"][-1]:.5f} | Val Loss: {loss_stats[\"val\"][-1]:.5f} | Avg Grad: {gradient_norm_epoch}')\n",
    "                wandb.log({\"Train Loss\":loss_stats[\"train\"][-1], \"Val Loss\":loss_stats[\"val\"][-1], \"Average Gradient\": gradient_norm_epoch, \"Var-1\": loss_stats[\"var-1\"],\"Var-2\": loss_stats[\"var-2\"],\"Var-3\": loss_stats[\"var-3\"],\"Var-4\": loss_stats[\"var-4\"],\"Var-5\": loss_stats[\"var-5\"], \"Var-6\": loss_stats[\"var-6\"],\"Var-7\": loss_stats[\"var-7\"],\"Var-8\": loss_stats[\"var-8\"],\"Var-9\": loss_stats[\"var-9\"],\"Var-10\": loss_stats[\"var-10\"], 'custom_step': e})\n",
    "\n",
    "                val_epoch_loss = 0\n",
    "                val_epoch_acc = 0\n",
    "                gradient_norm_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cbe135bd",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 56.,  56.,  20.],\n",
      "        [ 48.,  51.,  12.],\n",
      "        [ 11.,  14.,   0.],\n",
      "        ...,\n",
      "        [  5.,   3., 176.],\n",
      "        [ 63.,  77., 130.],\n",
      "        [ 98.,  75.,  59.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "de56eda5",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: i5th1jc4 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tEPOCHS: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLEARNING_RATE: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tNUM_CLASSES: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tNUM_FEATURES: 3\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mas278\u001b[0m (\u001b[33mrice-and-shine\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/as278/580-PROJECT/wandb/run-20230423_133733-i5th1jc4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rice-and-shine/Data_Variance_Exp/runs/i5th1jc4' target=\"_blank\">super-sweep-1</a></strong> to <a href='https://wandb.ai/rice-and-shine/Data_Variance_Exp' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/rice-and-shine/Data_Variance_Exp/sweeps/gwuk8cde' target=\"_blank\">https://wandb.ai/rice-and-shine/Data_Variance_Exp/sweeps/gwuk8cde</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rice-and-shine/Data_Variance_Exp' target=\"_blank\">https://wandb.ai/rice-and-shine/Data_Variance_Exp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/rice-and-shine/Data_Variance_Exp/sweeps/gwuk8cde' target=\"_blank\">https://wandb.ai/rice-and-shine/Data_Variance_Exp/sweeps/gwuk8cde</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rice-and-shine/Data_Variance_Exp/runs/i5th1jc4' target=\"_blank\">https://wandb.ai/rice-and-shine/Data_Variance_Exp/runs/i5th1jc4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aae52ed0df254b1e9039f4a173d95236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 0.59599 | Val Loss: 0.52408 | Avg Grad: 25.47471145668813\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">super-sweep-1</strong> at: <a href='https://wandb.ai/rice-and-shine/Data_Variance_Exp/runs/i5th1jc4' target=\"_blank\">https://wandb.ai/rice-and-shine/Data_Variance_Exp/runs/i5th1jc4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230423_133733-i5th1jc4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run i5th1jc4 errored: NameError(\"name 'neighbor_norms' is not defined\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run i5th1jc4 errored: NameError(\"name 'neighbor_norms' is not defined\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Exiting.\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, wandb_trainer_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a9c682",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda6af03",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd81642e",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcd6c71",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0bcee2",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856f8672",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f384ae73",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myproject",
   "language": "python",
   "name": "myproject"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
