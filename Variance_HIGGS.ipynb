{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3575733f",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "#### figure out a way to make the model deeper \n",
    "#### consider small model as low rank representation and project higher. (using outer product) train only small number of outer product parameters - https://arxiv.org/pdf/2012.13255.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73ab1cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler  \n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import wandb\n",
    "   \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import wandb\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "456fd859",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 0\n",
    "if dataset == 0:\n",
    "    df=pd.read_csv(\"HIGGS.csv\",header=None, nrows=50000)\n",
    "    # df=pd.read_csv(\"HIGGS.csv\",header=None)\n",
    "    df.head()\n",
    "elif dataset == 1:\n",
    "    df=pd.read_csv(\"SUSY.csv\",header=None, nrows=600000)\n",
    "    df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64c99fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 1:]  \n",
    "y = df.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ccab5b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74</td>\n",
       "      <td>85</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73</td>\n",
       "      <td>84</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72</td>\n",
       "      <td>83</td>\n",
       "      <td>121</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>81</td>\n",
       "      <td>119</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>69</td>\n",
       "      <td>80</td>\n",
       "      <td>118</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244672</th>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>49</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244703</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244719</th>\n",
       "      <td>62</td>\n",
       "      <td>64</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244720</th>\n",
       "      <td>58</td>\n",
       "      <td>61</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244848</th>\n",
       "      <td>103</td>\n",
       "      <td>106</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51444 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0    1    2  3\n",
       "0        74   85  123  1\n",
       "1        73   84  122  1\n",
       "2        72   83  121  1\n",
       "3        70   81  119  1\n",
       "5        69   80  118  1\n",
       "...     ...  ...  ... ..\n",
       "244672   73   73   49  2\n",
       "244703   19   19   19  2\n",
       "244719   62   64   35  2\n",
       "244720   58   61   29  2\n",
       "244848  103  106   51  2\n",
       "\n",
       "[51444 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "930e95a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=69)\n",
    "\n",
    "# Split train into train-val\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.1, stratify=y_trainval, random_state=21)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1fbd63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_val, y_val = np.array(X_val), np.array(y_val)\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7fa40f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "    def getBatch(self, indices, yclass = -1):\n",
    "        if yclass >= 0:\n",
    "            return self.X_data[self.y_data == yclass], self.y_data[self.y_data == yclass]\n",
    "        else:\n",
    "            return self.X_data[(indices)], self.y_data[(indices)]\n",
    "    \n",
    "    def getSplitbyClass(self, indices, yclass):\n",
    "        return self.X_data[self.y_data == yclass][indices], self.y_data[self.y_data == yclass][indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46dd35ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "512e95d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ClassifierDataset(X_data=X_train.astype(float), y_data=y_train)\n",
    "val_dataset = ClassifierDataset(X_data=X_val.astype(float), y_data=y_val)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=len(train_dataset), shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56b64256",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(train_dataset.X_data, device = device)\n",
    "y = torch.tensor(train_dataset.y_data, device = device)\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 512\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_FEATURES = X.shape[1]\n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d32374eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "print(y.int())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43e55274",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'grid'\n",
    "    }\n",
    "\n",
    "parameters_dict = {\n",
    "    \"LEARNING_RATE\": {\n",
    "        'value': LEARNING_RATE\n",
    "        },\n",
    "    \"NUM_FEATURES\": {\n",
    "        'value': NUM_FEATURES\n",
    "        },\n",
    "    \"NUM_CLASSES\": {\n",
    "        'value': NUM_CLASSES\n",
    "        },\n",
    "    \"EPOCHS\": {\n",
    "        'value': EPOCHS\n",
    "        },\n",
    "    }\n",
    "# \n",
    "sweep_config['parameters'] = parameters_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dfd76aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: kndbnekn\n",
      "Sweep URL: https://wandb.ai/rice-and-shine/Data_Variance_Exp/sweeps/kndbnekn\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"Data_Variance_Exp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "26083968",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulticlassClassification(nn.Module):\n",
    "    def __init__(self, num_feature, num_class):\n",
    "        super(MulticlassClassification, self).__init__()\n",
    "        \n",
    "        self.layer_1 = nn.Linear(num_feature, 2048)\n",
    "        self.layer_2 = nn.Linear(2048, 512)\n",
    "        self.layer_3 = nn.Linear(512, 128)\n",
    "        self.layer_out = nn.Linear(128, num_class)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.batchnorm1 = nn.InstanceNorm1d(2048)\n",
    "        self.batchnorm2 = nn.InstanceNorm1d(512)\n",
    "        self.batchnorm3 = nn.InstanceNorm1d(128)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.layer_2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.layer_3(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        \n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f8fdb2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neighbors = torch.linalg.norm(X.unsqueeze(1) - X.unsqueeze(0), dim=2).argsort(dim=1)[:, :33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "66d7d00d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 38808, 27056,  ..., 12742,  5437, 18427],\n",
       "        [    1, 38909, 25967,  ..., 24233, 11397,  4644],\n",
       "        [    2, 37792, 40271,  ...,  9303, 32798, 22147],\n",
       "        ...,\n",
       "        [41152,  1855, 23863,  ..., 21431,  5212, 21239],\n",
       "        [41153, 33173, 36497,  ..., 39413, 18052,  6038],\n",
       "        [41154, 12283, 26185,  ..., 27497, 17250, 31485]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "00ddd3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def compute_loss_stateless_model (params, buffers, sample, target):\n",
    "    batch = sample.unsqueeze(0)\n",
    "    targets = target.unsqueeze(0)\n",
    "\n",
    "    predictions = fmodel(params, buffers, batch) \n",
    "    loss = torch.nn.functional.cross_entropy(predictions, targets)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def compute_next_batch(X, y, batch_norm):\n",
    "    highest_grad_points = batch_norm.argsort(descending=True)[:32]\n",
    "    next_batch = X[highest_grad_points]\n",
    "    # neighbors = torch.linalg.norm(X.unsqueeze(0) - next_batch.unsqueeze(1), dim=2).argsort(dim=1)[:, :32]\n",
    "    # neigh_norms = 0\n",
    "    # for neigh  in neighbors:\n",
    "    #     neigh_grads = batch_norm[neigh]\n",
    "    #     neigh_norms += sum(neigh_grads)/len(neigh_grads)\n",
    "    \n",
    "    # neigh_norms /= len(neighbors)\n",
    "\n",
    "\n",
    "    x0=next_batch[y==0]\n",
    "    x1=next_batch[y==1]\n",
    "\n",
    "    batch_norm0=batch_norm[y==0]\n",
    "    batch_norm1=batch_norm[y==1]\n",
    "\n",
    "    next_batch_0 = torch.linalg.norm(X.unsqueeze(0)-x0.unsqueeze(1),dim=2).argsort(dim=1)[:,1:32+1]\n",
    "    next_batch_0=torch.reshape(next_batch_0,(-1,))\n",
    "\n",
    "    next_batch_1 = torch.linalg.norm(X.unsqueeze(0)-x1.unsqueeze(1),dim=2).argsort(dim=1)[:,1:32+1]\n",
    "    next_batch_1=torch.reshape(next_batch_1,(-1,))\n",
    "\n",
    "    \n",
    "    \n",
    "    next_batch_weights_0=torch.cat(tuple(batch_norm0.repeat(32,1).T))\n",
    "    next_batch_weights_1=torch.cat(tuple(batch_norm1.repeat(32,1).T))\n",
    "\n",
    "\n",
    "    ## weighted selection of next batch.\n",
    "    next_batch_0 = next_batch_0[next_batch_weights_0.multinomial(32)]\n",
    "    next_batch_1 = next_batch_1[next_batch_weights_1.multinomial(32)]\n",
    "    neigh_norms = 0\n",
    "    for neigh  in next_batch_0:\n",
    "        neigh_grads = batch_norm[neigh]\n",
    "        neigh_norms += sum(neigh_grads)/len(neigh_grads)\n",
    "    \n",
    "    \n",
    "\n",
    "    for neigh  in next_batch_1:\n",
    "        neigh_grads = batch_norm[neigh]\n",
    "        neigh_norms += sum(neigh_grads)/len(neigh_grads)\n",
    "    \n",
    "    neigh_norms /= (len(next_batch_1)+len(next_batch_0))\n",
    "    \n",
    "    # del neighbors\n",
    "\n",
    "    del highest_grad_points\n",
    "    gc.collect()\n",
    "    return neigh_norms\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e452af48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MulticlassClassification(\n",
       "  (layer_1): Linear(in_features=28, out_features=2048, bias=True)\n",
       "  (layer_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (layer_3): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (layer_out): Linear(in_features=128, out_features=2, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (batchnorm1): InstanceNorm1d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  (batchnorm2): InstanceNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  (batchnorm3): InstanceNorm1d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "from functorch import make_functional_with_buffers, vmap, grad\n",
    "\n",
    "base_model = MulticlassClassification(num_feature = NUM_FEATURES, num_class=NUM_CLASSES)\n",
    "base_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fc3a4811",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_compute_grad = grad(compute_loss_stateless_model)\n",
    "ft_compute_sample_grad = vmap(ft_compute_grad, in_dims=(None, None, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f205e4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_stats = {\n",
    "    'train': [],\n",
    "    \"val\": [],\n",
    "    \"grad\": [],\n",
    "    \"var-1\":[],\n",
    "    \"var-2\":[],\n",
    "    \"var-3\":[],\n",
    "    \"var-4\":[],\n",
    "    \"var-5\":[],\n",
    "    \"var-6\":[],\n",
    "    \"var-7\":[],\n",
    "    \"var-8\":[],\n",
    "    \"var-9\":[],\n",
    "    \"var-10\":[],\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "155d202a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a4ff4de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variance_batch(max_grad_data_point,y_max_grad_data_point):\n",
    "\n",
    "    new_batch=[]\n",
    "    # print(max_grad_data_point.shape)\n",
    "    d=max_grad_data_point.shape[1]\n",
    "\n",
    "    for i in range(50,60):\n",
    "        for j in range(100):\n",
    "            random_point=np.random.rand(d)\n",
    "            norm=np.linalg.norm(random_point)\n",
    "            random_point=random_point*(i/norm)\n",
    "            new_batch.append(random_point)\n",
    "            # print(torch.linalg.norm(random_point))\n",
    "    return torch.tensor(new_batch),torch.tensor([y_max_grad_data_point]*1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "608274cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "l,m=get_variance_batch(X[[222]],y[[222]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5fc02f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wandb_trainer_function(config=None):\n",
    "    # INITIALIZE NEW WANDB RUN\n",
    "    with wandb.init(config=config) as run:\n",
    "        #USING THE CONFIG TO SET THE HYPERPARAMETERS FOR EACH RUN\n",
    "        config = wandb.config\n",
    "        wandb.define_metric(\"custom_step\")\n",
    "        wandb.define_metric(\"Neighbor Gradient\", step_metric='custom_step')\n",
    "        wandb.define_metric(\"Average Gradient\", step_metric='custom_step')\n",
    "        wandb.define_metric(\"Train Loss\", step_metric='custom_step')\n",
    "        wandb.define_metric(\"Val Loss\", step_metric='custom_step')\n",
    "        \n",
    "\n",
    "        run.name = \"NN-HIGGS-50:60-Variance\"\n",
    "\n",
    "        model = copy.deepcopy(base_model)\n",
    "\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "        val_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "        global fmodel\n",
    "\n",
    "        print(\"Begin training.\")\n",
    "        train_epoch_loss=0\n",
    "        val_epoch_loss = 0\n",
    "        gradient_norm_epoch = 0\n",
    "        for e in tqdm(range(1, EPOCHS+1)):\n",
    "            torch.cuda.empty_cache()\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            pred = model(X.float())\n",
    "\n",
    "            train_loss = criterion(pred, y.long())\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            fmodel, params, buffers = make_functional_with_buffers(model)\n",
    "\n",
    "            ### use functorch to calculate per sample gradient\n",
    "            ft_per_sample_grads = ft_compute_sample_grad(params, buffers, X.float(), y.long())\n",
    "\n",
    "            ### calculate norm of the gradient and use it to compute next batch\n",
    "            batch_norm = torch.zeros(X.size(0), device=device)\n",
    "            for item in ft_per_sample_grads:\n",
    "                batch_norm +=  torch.linalg.norm(item, dim=tuple(range(1,len(item.shape))))\n",
    "            gradient_norm_epoch = batch_norm.sum().item()/X.size(0)\n",
    "\n",
    "            ###CODE FOR VARIANCE BATCH EXPERIMENT STARTS HERE\n",
    "            max_grad_data_point=batch_norm.argsort(descending=True)[0:1]\n",
    "            \n",
    "            variance_batch, y_max_grad_data_point=get_variance_batch(X[max_grad_data_point],y[max_grad_data_point])\n",
    "\n",
    "            ft_per_sample_grads_exp = ft_compute_sample_grad(params, buffers, variance_batch.float(), y_max_grad_data_point.long())\n",
    "\n",
    "            batch_norm_exp = torch.zeros(variance_batch.size(0), device=device)\n",
    "            for item in ft_per_sample_grads_exp:\n",
    "                batch_norm_exp +=  torch.linalg.norm(item, dim=tuple(range(1,len(item.shape))))\n",
    "            \n",
    "            gradient_norm_epoch_exp=torch.zeros(10, device=device)\n",
    "\n",
    "            for i in range(10):\n",
    "                gradient_norm_epoch_exp[i]=batch_norm_exp[i:i+100].sum().item()/100\n",
    "                loss_stats[f'var-{i+1}']=gradient_norm_epoch_exp[i]\n",
    "\n",
    "            ###CODE FOR VARIANCE BATCH EXPERIMENT ENDS HERE\n",
    "            del ft_per_sample_grads\n",
    "            del fmodel\n",
    "            del params\n",
    "            del buffers\n",
    "            gc.collect()\n",
    "            \n",
    "            ## compute neighbors of points that give largest gradients, and see if their gradients are higher in general\n",
    "            # neighbor_norms = compute_next_batch(X, y, batch_norm)\n",
    "\n",
    "\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                for X_val, y_val in val_loader:\n",
    "                    val_pred = model(X_val.float())\n",
    "                    val_loss = criterion(val_pred, y_val.long())\n",
    "                    val_epoch_loss += val_loss.item()\n",
    "\n",
    "\n",
    "                # TRAIN LOSS AND ACCURACY\n",
    "                loss_stats['train'].append(train_loss)\n",
    "                loss_stats['val'].append(val_epoch_loss/len(val_loader))\n",
    "                loss_stats['grad'].append(gradient_norm_epoch)\n",
    "\n",
    "\n",
    "                ## plot val loss and accuracy here. For train, standardise a subset for loss/accuracy\n",
    "                print(f'Epoch {e+0:03}: | Train Loss: {loss_stats[\"train\"][-1]:.5f} | Val Loss: {loss_stats[\"val\"][-1]:.5f} | Avg Grad: {gradient_norm_epoch}')\n",
    "                wandb.log({\"Train Loss\":loss_stats[\"train\"][-1], \"Val Loss\":loss_stats[\"val\"][-1], \"Average Gradient\": gradient_norm_epoch, \"Var-1\": loss_stats[\"var-1\"],\"Var-2\": loss_stats[\"var-2\"],\"Var-3\": loss_stats[\"var-3\"],\"Var-4\": loss_stats[\"var-4\"],\"Var-5\": loss_stats[\"var-5\"], \"Var-6\": loss_stats[\"var-6\"],\"Var-7\": loss_stats[\"var-7\"],\"Var-8\": loss_stats[\"var-8\"],\"Var-9\": loss_stats[\"var-9\"],\"Var-10\": loss_stats[\"var-10\"], 'custom_step': e})\n",
    "\n",
    "                val_epoch_loss = 0\n",
    "                val_epoch_acc = 0\n",
    "                gradient_norm_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cbe135bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 56.,  56.,  20.],\n",
      "        [ 48.,  51.,  12.],\n",
      "        [ 11.,  14.,   0.],\n",
      "        ...,\n",
      "        [  5.,   3., 176.],\n",
      "        [ 63.,  77., 130.],\n",
      "        [ 98.,  75.,  59.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "de56eda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: upgc3yl8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tEPOCHS: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLEARNING_RATE: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tNUM_CLASSES: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tNUM_FEATURES: 28\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/as278/580-PROJECT/wandb/run-20230423_143556-upgc3yl8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rice-and-shine/Data_Variance_Exp/runs/upgc3yl8' target=\"_blank\">zesty-sweep-1</a></strong> to <a href='https://wandb.ai/rice-and-shine/Data_Variance_Exp' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/rice-and-shine/Data_Variance_Exp/sweeps/kndbnekn' target=\"_blank\">https://wandb.ai/rice-and-shine/Data_Variance_Exp/sweeps/kndbnekn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rice-and-shine/Data_Variance_Exp' target=\"_blank\">https://wandb.ai/rice-and-shine/Data_Variance_Exp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/rice-and-shine/Data_Variance_Exp/sweeps/kndbnekn' target=\"_blank\">https://wandb.ai/rice-and-shine/Data_Variance_Exp/sweeps/kndbnekn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rice-and-shine/Data_Variance_Exp/runs/upgc3yl8' target=\"_blank\">https://wandb.ai/rice-and-shine/Data_Variance_Exp/runs/upgc3yl8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18df65921c59480999da7163095dff60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 0.76314 | Val Loss: 0.77983 | Avg Grad: 41.60123263888889\n",
      "Epoch 002: | Train Loss: 0.78225 | Val Loss: 0.73554 | Avg Grad: 36.807170138888885\n",
      "Epoch 003: | Train Loss: 0.73783 | Val Loss: 0.69472 | Avg Grad: 33.26524305555556\n",
      "Epoch 004: | Train Loss: 0.69589 | Val Loss: 0.69649 | Avg Grad: 30.52273611111111\n",
      "Epoch 005: | Train Loss: 0.69685 | Val Loss: 0.70784 | Avg Grad: 28.514027777777777\n",
      "Epoch 006: | Train Loss: 0.70776 | Val Loss: 0.70789 | Avg Grad: 27.108569444444445\n",
      "Epoch 007: | Train Loss: 0.70755 | Val Loss: 0.70024 | Avg Grad: 26.071727430555555\n",
      "Epoch 008: | Train Loss: 0.69961 | Val Loss: 0.69245 | Avg Grad: 25.200890625\n",
      "Epoch 009: | Train Loss: 0.69188 | Val Loss: 0.68914 | Avg Grad: 24.431663194444443\n",
      "Epoch 010: | Train Loss: 0.68869 | Val Loss: 0.69015 | Avg Grad: 23.75315625\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f21f646430e546c293ebf33b01da430f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.014 MB of 0.014 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Average Gradient</td><td>█▆▅▄▃▂▂▂▁▁</td></tr><tr><td>Train Loss</td><td>▇█▅▂▂▂▂▂▁▁</td></tr><tr><td>Val Loss</td><td>█▅▁▂▂▂▂▁▁▁</td></tr><tr><td>Var-1</td><td>█▆▄▄▄▄▃▂▁▁</td></tr><tr><td>Var-10</td><td>█▆▄▄▄▄▃▂▂▁</td></tr><tr><td>Var-2</td><td>█▆▄▄▄▄▃▂▂▁</td></tr><tr><td>Var-3</td><td>█▆▄▄▄▄▃▂▂▁</td></tr><tr><td>Var-4</td><td>█▆▄▄▄▄▃▂▂▁</td></tr><tr><td>Var-5</td><td>█▆▄▄▄▄▃▂▂▁</td></tr><tr><td>Var-6</td><td>█▆▄▄▄▄▃▂▂▁</td></tr><tr><td>Var-7</td><td>█▆▄▄▄▄▃▂▂▁</td></tr><tr><td>Var-8</td><td>█▆▄▄▄▄▃▂▂▁</td></tr><tr><td>Var-9</td><td>█▆▄▄▄▄▃▂▂▁</td></tr><tr><td>custom_step</td><td>▁▂▃▃▄▅▆▆▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Average Gradient</td><td>23.75316</td></tr><tr><td>Train Loss</td><td>0.68869</td></tr><tr><td>Val Loss</td><td>0.69015</td></tr><tr><td>Var-1</td><td>25.08708</td></tr><tr><td>Var-10</td><td>25.12664</td></tr><tr><td>Var-2</td><td>25.03853</td></tr><tr><td>Var-3</td><td>25.02548</td></tr><tr><td>Var-4</td><td>25.01478</td></tr><tr><td>Var-5</td><td>25.02056</td></tr><tr><td>Var-6</td><td>25.03536</td></tr><tr><td>Var-7</td><td>25.0341</td></tr><tr><td>Var-8</td><td>25.08329</td></tr><tr><td>Var-9</td><td>25.08924</td></tr><tr><td>custom_step</td><td>10</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">zesty-sweep-1</strong> at: <a href='https://wandb.ai/rice-and-shine/Data_Variance_Exp/runs/upgc3yl8' target=\"_blank\">https://wandb.ai/rice-and-shine/Data_Variance_Exp/runs/upgc3yl8</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230423_143556-upgc3yl8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Exiting.\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, wandb_trainer_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a9c682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda6af03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd81642e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcd6c71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0bcee2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856f8672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f384ae73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myproject",
   "language": "python",
   "name": "myproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
