{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3575733f",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "#### figure out a way to make the model deeper \n",
    "#### consider small model as low rank representation and project higher. (using outer product) train only small number of outer product parameters - https://arxiv.org/pdf/2012.13255.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73ab1cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import wandb\n",
    "   \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import wandb\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f875dfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Link to dataset: https://archive.ics.uci.edu/ml/datasets/DeliciousMIL%3A+A+Data+Set+for+Multi-Label+Multi-Instance+Learning+with+Instance+Labels#\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def createDataset(path):\n",
    "    file = open(path, \"r\")\n",
    "    dataset = []\n",
    "    split = file.read().splitlines()\n",
    " \n",
    "    for i, line in enumerate(split):\n",
    "        row = line.split(\" \")\n",
    "        while \"\" in row:\n",
    "            row.remove(\"\")\n",
    "        for i, item in enumerate(row): \n",
    "            try:\n",
    "                row[i] = float(item.split(\":\")[1])\n",
    "            except:\n",
    "                row[i] = int(item)\n",
    "        dataset.append(row)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd9c17e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(createDataset('../Data/Skin/skin_nonskin.txt'))\n",
    "dataset.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccab5b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>74.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>123.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>73.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>122.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>72.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>70.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>119.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>70.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>119.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245052</th>\n",
       "      <td>2</td>\n",
       "      <td>163.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245053</th>\n",
       "      <td>2</td>\n",
       "      <td>163.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245054</th>\n",
       "      <td>2</td>\n",
       "      <td>163.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245055</th>\n",
       "      <td>2</td>\n",
       "      <td>163.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245056</th>\n",
       "      <td>2</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>245057 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0      1      2      3\n",
       "0       1   74.0   85.0  123.0\n",
       "1       1   73.0   84.0  122.0\n",
       "2       1   72.0   83.0  121.0\n",
       "3       1   70.0   81.0  119.0\n",
       "4       1   70.0   81.0  119.0\n",
       "...    ..    ...    ...    ...\n",
       "245052  2  163.0  162.0  112.0\n",
       "245053  2  163.0  162.0  112.0\n",
       "245054  2  163.0  162.0  112.0\n",
       "245055  2  163.0  162.0  112.0\n",
       "245056  2  255.0  255.0  255.0\n",
       "\n",
       "[245057 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89eb5290",
   "metadata": {},
   "outputs": [],
   "source": [
    "class skin(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04643aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train shape :  (196045, 3)\n",
      "y train label shape :  (196045,)\n",
      "X valid  shape :  (49012, 3)\n",
      "y valid shape:  (49012,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler  \n",
    "\n",
    "X_train, X_valid, y_train, y_valid = \\\n",
    "    train_test_split(dataset.loc[:, 1:], dataset.loc[:, 0], test_size=0.2, random_state=42)\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_valid = scaler.transform(X_valid)\n",
    "\n",
    "y_train_proc = y_train - min(min(y_train), min(y_valid))\n",
    "y_valid_proc = y_valid - min(min(y_train), min(y_valid))\n",
    "\n",
    "X_train, y_train = np.array(X_train), np.array(y_train_proc)\n",
    "X_valid, y_valid = np.array(X_valid), np.array(y_valid_proc)\n",
    "\n",
    "\n",
    "print('X train shape : ', X_train.shape)\n",
    "print('y train label shape : ', y_train.shape)\n",
    "print('X valid  shape : ', X_valid.shape)\n",
    "print('y valid shape: ', y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "512e95d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = skin(X=X_train, y=y_train)\n",
    "val_dataset = skin(X=X_valid, y=y_valid)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=len(train_dataset), shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56b64256",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(train_dataset.X, device = device)\n",
    "y = torch.tensor(train_dataset.y, device = device)\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 512\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_FEATURES = X.shape[1]\n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43e55274",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'grid'\n",
    "    }\n",
    "\n",
    "parameters_dict = {\n",
    "    \"LEARNING_RATE\": {\n",
    "        'value': LEARNING_RATE\n",
    "        },\n",
    "    \"NUM_FEATURES\": {\n",
    "        'value': NUM_FEATURES\n",
    "        },\n",
    "    \"NUM_CLASSES\": {\n",
    "        'value': NUM_CLASSES\n",
    "        },\n",
    "    \"EPOCHS\": {\n",
    "        'value': EPOCHS\n",
    "        },\n",
    "    }\n",
    "# \n",
    "sweep_config['parameters'] = parameters_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfd76aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 4wmv17t5\n",
      "Sweep URL: https://wandb.ai/rice-and-shine/sanity-experiments/sweeps/4wmv17t5\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"sanity-experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26083968",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulticlassClassification(nn.Module):\n",
    "    def __init__(self, num_feature, num_class):\n",
    "        super(MulticlassClassification, self).__init__()\n",
    "        \n",
    "        self.layer_1 = nn.Linear(num_feature, 2048)\n",
    "        self.layer_2 = nn.Linear(2048, 512)\n",
    "        self.layer_3 = nn.Linear(512, 128)\n",
    "        self.layer_out = nn.Linear(128, num_class)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.batchnorm1 = nn.InstanceNorm1d(2048)\n",
    "        self.batchnorm2 = nn.InstanceNorm1d(512)\n",
    "        self.batchnorm3 = nn.InstanceNorm1d(128)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.layer_2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.layer_3(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        \n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6b4dd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences shape on PyTorch :  torch.Size([196045, 3])\n",
      "labels shape on PyTorch :  torch.Size([196045])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "sentences, labels = next(dataiter)\n",
    "\n",
    "print('sentences shape on PyTorch : ', sentences.size())\n",
    "print('labels shape on PyTorch : ', labels.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fdb2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = torch.linalg.norm(X.unsqueeze(1) - X.unsqueeze(0), dim=2).argsort(dim=1)[:, :33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d7d00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00ddd3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def compute_loss_stateless_model (params, buffers, sample, target):\n",
    "    batch = sample.unsqueeze(0)\n",
    "    targets = target.unsqueeze(0)\n",
    "\n",
    "    predictions = fmodel(params, buffers, batch) \n",
    "    loss = torch.nn.functional.cross_entropy(predictions, targets)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def compute_next_batch(X, y, batch_norm, neighbors):\n",
    "    highest_grad_points = batch_norm.argsort(descending=True)[:32]\n",
    "    next_batch = X[highest_grad_points]\n",
    "    neighbors = torch.linalg.norm(X.unsqueeze(0) - next_batch.unsqueeze(1), dim=2).argsort(dim=1)[:, :32]\n",
    "    neigh_norms = 0\n",
    "    for neigh  in neighbors:\n",
    "        neigh_grads = batch_norm[neigh]\n",
    "        neigh_norms += sum(neigh_grads)/len(neigh_grads)\n",
    "    \n",
    "    neigh_norms /= len(neighbors)\n",
    "    del neighbors\n",
    "    del highest_grad_points\n",
    "    gc.collect()\n",
    "    return neigh_norms\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e452af48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MulticlassClassification(\n",
       "  (layer_1): Linear(in_features=3, out_features=2048, bias=True)\n",
       "  (layer_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (layer_3): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (layer_out): Linear(in_features=128, out_features=2, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (batchnorm1): InstanceNorm1d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  (batchnorm2): InstanceNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  (batchnorm3): InstanceNorm1d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "from functorch import make_functional_with_buffers, vmap, grad\n",
    "\n",
    "base_model = MulticlassClassification(num_feature = NUM_FEATURES, num_class=NUM_CLASSES)\n",
    "base_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc3a4811",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_compute_grad = grad(compute_loss_stateless_model)\n",
    "ft_compute_sample_grad = vmap(ft_compute_grad, in_dims=(None, None, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f205e4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_stats = {\n",
    "    'train': [],\n",
    "    \"val\": [],\n",
    "    \"grad\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fc02f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wandb_trainer_function(config=None):\n",
    "    # INITIALIZE NEW WANDB RUN\n",
    "    with wandb.init(config=config) as run:\n",
    "        #USING THE CONFIG TO SET THE HYPERPARAMETERS FOR EACH RUN\n",
    "        config = wandb.config\n",
    "        wandb.define_metric(\"custom_step\")\n",
    "        wandb.define_metric(\"Neighbor Gradient\", step_metric='custom_step')\n",
    "        wandb.define_metric(\"Average Gradient\", step_metric='custom_step')\n",
    "        wandb.define_metric(\"Train Loss\", step_metric='custom_step')\n",
    "        wandb.define_metric(\"Val Loss\", step_metric='custom_step')\n",
    "        \n",
    "\n",
    "        run.name = \"NN-Covtype\"\n",
    "\n",
    "        model = copy.deepcopy(base_model)\n",
    "\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "        val_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "        global fmodel\n",
    "\n",
    "        print(\"Begin training.\")\n",
    "        train_epoch_loss=0\n",
    "        val_epoch_loss = 0\n",
    "        gradient_norm_epoch = 0\n",
    "        for e in tqdm(range(1, EPOCHS+1)):\n",
    "            torch.cuda.empty_cache()\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(X.float())\n",
    "\n",
    "            train_loss = criterion(pred, y)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            fmodel, params, buffers = make_functional_with_buffers(model)\n",
    "\n",
    "            ### use functorch to calculate per sample gradient\n",
    "            ft_per_sample_grads = ft_compute_sample_grad(params, buffers, X.float(), y)\n",
    "\n",
    "            ### calculate norm of the gradient and use it to compute next batch\n",
    "            batch_norm = torch.zeros(X.size(0), device=device)\n",
    "            for item in ft_per_sample_grads:\n",
    "                batch_norm +=  torch.linalg.norm(item, dim=tuple(range(1,len(item.shape))))\n",
    "            gradient_norm_epoch = batch_norm.sum().item()/X.size(0)\n",
    "            del ft_per_sample_grads\n",
    "            del fmodel\n",
    "            del params\n",
    "            del buffers\n",
    "            gc.collect()\n",
    "            ## compute neighbors of points that give largest gradients, and see if their gradients are higher in general\n",
    "            neighbor_norms = compute_next_batch(X, y, batch_norm)\n",
    "\n",
    "\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                for X_val, y_val in val_loader:\n",
    "                    val_pred = model(X_val.float())\n",
    "                    val_loss = criterion(val_pred, y_val)\n",
    "                    val_epoch_loss += val_loss.item()\n",
    "\n",
    "\n",
    "                # TRAIN LOSS AND ACCURACY\n",
    "                loss_stats['train'].append(train_loss)\n",
    "                loss_stats['val'].append(val_epoch_loss/len(val_loader))\n",
    "                loss_stats['grad'].append(gradient_norm_epoch)\n",
    "\n",
    "\n",
    "                ## plot val loss and accuracy here. For train, standardise a subset for loss/accuracy\n",
    "                print(f'Epoch {e+0:03}: | Train Loss: {loss_stats[\"train\"][-1]:.5f} | Val Loss: {loss_stats[\"val\"][-1]:.5f} | Avg Grad: {gradient_norm_epoch} | Neighbor Grad: {neighbor_norms}')\n",
    "                wandb.log({\"Train Loss\":loss_stats[\"train\"][-1], \"Val Loss\":loss_stats[\"val\"][-1], \"Neighbor Gradient\": neighbor_norms, \"Average Gradient\": gradient_norm_epoch, 'custom_step': e})\n",
    "\n",
    "                val_epoch_loss = 0\n",
    "                val_epoch_acc = 0\n",
    "                gradient_norm_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de56eda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: efxo694f with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tEPOCHS: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLEARNING_RATE: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tNUM_CLASSES: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tNUM_FEATURES: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maw82\u001b[0m (\u001b[33mrice-and-shine\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/aw82/LSHBatching/Notebooks/wandb/run-20230403_181644-efxo694f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/rice-and-shine/sanity-experiments/runs/efxo694f\" target=\"_blank\">earthy-sweep-1</a></strong> to <a href=\"https://wandb.ai/rice-and-shine/sanity-experiments\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/rice-and-shine/sanity-experiments/sweeps/mr26ibdr\" target=\"_blank\">https://wandb.ai/rice-and-shine/sanity-experiments/sweeps/mr26ibdr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.017106294631958008,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 23,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 10,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67bd6e56366444fa88b68506b9309497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 0.63223 | Val Loss: 0.45689 | Avg Grad: 23.286056772679743 | Neighbor Grad: 76.15120697021484\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, wandb_trainer_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a9c682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda6af03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd81642e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcd6c71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0bcee2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856f8672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f384ae73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
